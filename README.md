# ç»“åˆ3Då½±åƒä¸“å®¶æ¨¡å‹çš„å¤šæ¨¡æ€é—®ç­”
ç›®å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨2Då›¾åƒä¸Šçš„ç†è§£èƒ½åŠ›å·²ç»å–å¾—äº†æå¤§çš„è¿›å±•ï¼Œä½†æ˜¯å¯¹äº3Då½±åƒä¸­ç±»ä¼¼ç»“èŠ‚è¿™ç§å¾®å°çš„ç—…ç¶è¿˜æ²¡æœ‰æˆç†Ÿçš„è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡å¦‚æ­¤ï¼Œç±»ä¼¼3Dè‚ºç»“èŠ‚æ£€æµ‹çš„æ¨¡å‹å½±åƒæ¨¡å‹å¯ä»¥ä½œä¸ºä¸“å®¶æ¨¡å‹æä¾›ç»™LLMå½±åƒä¿¡æ¯ï¼ŒLLMç»™å‡ºè¯Šæ–­æ„è§ã€‚è¿›ä¸€æ­¥æ¥è¯´ï¼Œå¯ä»¥è¾“å…¥ä»»æ„ç±»å‹çš„3Då½±åƒï¼Œè®©MLLMå†³å®šè°ƒç”¨å¯¹åº”çš„ä¸“å®¶æ¨¡å‹ï¼Œç„¶åç»“åˆä¸“å®¶æ¨¡å‹ç»“æœæ›´å¥½çš„å›ç­”é—®é¢˜ã€‚
## Demo

## Contents

- [Install](#install)
- [Model Download](#model-download)
- [Serving](#serving)
- [Evaluation](#evaluation)

## Install

1. Clone this repository and navigate to LLaVA-Med folder
```bash
https://github.com/microsoft/LLaVA-Med.git
cd LLaVA-Med
```

2. Install Package: Create conda environment

```Shell
conda create -n llava-med python=3.10 -y
conda activate llava-med
pip install --upgrade pip  # enable PEP 660 support
pip install -e .
pip install -e ".[train]"
pip install flash-attn --no-build-isolation
```

## Model Download


 Model Descriptions | ğŸ¤— Huggingface Hub | 
| --- | ---: |
| LLaVA-Med v1.5 | [microsoft/llava-med-v1.5-mistral-7b](https://huggingface.co/microsoft/llava-med-v1.5-mistral-7b) |



## Serving

### Web UI

#### Launch a controller
```Shell
python -m llava.serve.controller --host 0.0.0.0 --port 10000
```

#### Launch a model worker
```Shell
python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path microsoft/llava-med-v1.5-mistral-7b --multi-modal
```
Wait until the process finishes loading the model and you see "Uvicorn running on ...".

#### Launch a model worker (Multiple GPUs, when GPU VRAM <= 24GB)

If your the VRAM of your GPU is less than 24GB (e.g., RTX 3090, RTX 4090, etc.), you may try running it with multiple GPUs.

```Shell
python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path microsoft/llava-med-v1.5-mistral-7b --multi-modal --num-gpus 2
```
Wait until the process finishes loading the model and you see "Uvicorn running on ...".


#### Send a test message
```Shell
python -m llava.serve.test_message --model-name llava-med-v1.5-mistral-7b --controller http://localhost:10000
```

#### Launch a gradio web server.
```Shell
python -m llava.serve.gradio_web_server --controller http://localhost:10000
```
#### You can open your browser and chat with a model now.


## Evaluation





## Related Projects

- [LLaVA](https://llava-vl.github.io/)
- [LLaVA-Med](https://github.com/microsoft/LLaVA-Med)
- [MMedAgent](https://github.com/Wangyixinxin/MMedAgent)
- [VLM-Radiology-Agent-Framework](https://github.com/Project-MONAI/VLM-Radiology-Agent-Framework)